{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c520588-2e7a-4309-82ec-6258ebd24b63",
   "metadata": {},
   "source": [
    "# Group project : PySpark \n",
    "\n",
    "#### Bilal Kostet, Antoine Somerhausen, Pierre Hosselet, Pacome Van Overschelde, Romain Vandepopeliere - Group 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c00fee-0111-445b-a956-997d57c2d2e6",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "0. [Starting PySpark](#Starting-PySpark)\n",
    "0. [Loading and preparing data : adding the zone column](#Loading-and-preparing-data)\n",
    "0. [Question 1](#Question-1)\n",
    "0. [Question 2](#Question-2)\n",
    "    * [Average speed, travel time and travel distance by zone](#2.1-Dynamics)\n",
    "    * [Average visited zones and exchanges](#2.2-Exchanges)\n",
    "0. [Cleaning up](#Cleaning-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec42740-f613-4ff2-9387-7417cd3677d9",
   "metadata": {},
   "source": [
    "## Starting PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ad145c-caeb-4471-bda9-e6a74ead6859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is needed to start a Spark session from the notebook\n",
    "#You may adjust the memory used by the driver program based on your machine's settings\n",
    "import os \n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=2g  pyspark-shell\"\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69adaa1a-3ce9-4e28-8bbd-ffc27ff3e119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 21:45:32 WARN Utils: Your hostname, romain-XPS-13-9300 resolves to a loopback address: 127.0.1.1; using 192.168.0.20 instead (on interface wlp0s20f3)\n",
      "23/01/19 21:45:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 21:45:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Start Spark in LOCAL mode\n",
    "# -------------------------------\n",
    "\n",
    "#The following lines are just there to allow this cell to be re-executed multiple times:\n",
    "#if a spark session was already started, we stop it before starting a new one\n",
    "#(there can be only one spark context per jupyter notebook)\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"demoRDD\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4be4d474-9ffc-4268-ad14-6e697db5b383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.app.startTime', '1674161132836'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', '192.168.0.20'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.app.id', 'local-1674161134539'),\n",
       " ('spark.driver.memory', '2g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.driver.port', '45325'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.submitTime', '1674161132628'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'demoRDD')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that we have a working spark context, print its configuration\n",
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420828a-4364-4dab-8668-aad5633ab906",
   "metadata": {},
   "source": [
    "[back to Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b98d35-bfde-4c7c-b651-2e38cdc59b11",
   "metadata": {},
   "source": [
    "## Loading and preparing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518cfe41-9862-4fa6-9277-f5755c779951",
   "metadata": {},
   "source": [
    "Let us import the packages and the functions that we'll need throughout this work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "481de666-b75c-4129-9b9a-faeffc715171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time \n",
    "import geopy.distance\n",
    "import pandas as pd \n",
    "import shapely.geometry as sg\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import udf, col, lead, mean , sum\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.sql.window import Window  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416ad65-b155-409c-845a-1caf2f28e6be",
   "metadata": {},
   "source": [
    "Spark can read .csv files and put them in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f18150f-e021-498d-aa82-434a0ca9e40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "drivers =  spark.read.csv('drivers.csv', header=True, inferSchema=True)   #inferSchema = True, in order not to have strings only in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f93935-d407-47ed-804f-5a422726517b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----------+----------+\n",
      "|  driver|          timestamp|  latitude| longitude|\n",
      "+--------+-------------------+----------+----------+\n",
      "|c473205b|2017-08-31 14:24:25|-12.106778|-76.998078|\n",
      "|a0f3b4e1|2017-08-31 14:24:26|-12.103913|-76.963727|\n",
      "|1236f9fe|2017-08-31 14:24:26|-12.133777|-77.004266|\n",
      "|ae4a06a2|2017-08-31 14:24:26|-12.085963|-76.987582|\n",
      "|ab7a6c63|2017-08-31 14:24:26|-12.072973|-77.061448|\n",
      "|5ee73484|2017-08-31 14:24:26|-12.067694|-77.068442|\n",
      "|4fff90cb|2017-08-31 14:24:26|-12.144308|-76.989234|\n",
      "|d892d208|2017-08-31 14:24:26|-16.401221|-71.499513|\n",
      "|e9f90dfb|2017-08-31 14:24:10|-12.063665|-76.963254|\n",
      "|c1719f8d|2017-08-31 14:24:10|-12.070187|-76.994167|\n",
      "|4c299505|2017-08-31 14:24:11| -11.96459|-77.015983|\n",
      "|49f033bd|2017-08-31 14:24:11|  -12.0906|-77.069808|\n",
      "|749df32a|2017-08-31 14:24:11|-12.109964| -76.97535|\n",
      "|f8d5c453|2017-08-31 14:24:11|-12.080683|-77.036188|\n",
      "|4314c58a|2017-08-31 14:24:11|-16.404571|-71.519775|\n",
      "|914f7296|2017-08-31 14:24:11|-12.218541|-76.928484|\n",
      "|ee744228|2017-08-31 14:24:30|-12.120202|-77.035704|\n",
      "|353c20b9|2017-08-31 14:24:31|-12.119177|-76.997462|\n",
      "|483e84f9|2017-08-31 14:24:31|-12.109511|-76.995862|\n",
      "|e26ff40f|2017-08-31 14:24:31|-12.130376|-77.017779|\n",
      "+--------+-------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- driver: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drivers.show()\n",
    "drivers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def2e94-357d-49e5-b979-5e0d9401e8e6",
   "metadata": {},
   "source": [
    "We also have to create the zones dataframe, based on the JSON file. This file is really small, and will be used in one function only. We assume that reading via PySpark is not necessary here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c2c11c-26a3-41ad-9d16-de34423676c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('zones.json') as f:\n",
    "    d = json.load(f) \n",
    "    \n",
    "zones = pd.json_normalize(d['zones'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8ed3d-dc7d-4be0-b636-c10c4c24a2d0",
   "metadata": {},
   "source": [
    "We will need to apply a defined function to the driver dataframe. Let us define the following function, that returns the zone in which a coordinate point is : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad2f3a7-2d0b-40e1-8d9c-621579c39f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_zone(latitude,longitude):\n",
    "    \n",
    "    #create a shapely Point object from the driver's coordinates\n",
    "    point = sg.Point(latitude,longitude)\n",
    "    \n",
    "    #checking if the point is in a given zone\n",
    "    for i in range(len(zones)):\n",
    "        polygon= sg.Polygon( (x['lat'],x['lng']) for x in zones['polygon'].iloc[i])\n",
    "        if polygon.contains(point):\n",
    "            return int(zones['id_zone'].iloc[i])\n",
    "        \n",
    "    #if no zone contains the point\n",
    "    return None   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39cf523-b657-4e23-939b-3a429dd8e2d8",
   "metadata": {},
   "source": [
    "In order to be used in a PySpark dataframe, this function has to be converted into a User Defined Function (= udf) following :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9e1e519-44bc-4fce-83dc-f4c2e32cf4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_zone_udf = udf(find_zone, IntegerType())  #The output is an integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec49e460-6190-4619-8122-896250bc1240",
   "metadata": {},
   "source": [
    "It is now time to simply generate the new column 'zone'. We will drop the null values since the points that do not belong to the zone dataset are not of interest for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a53d18a-4ad3-47cb-9024-ab26f500ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "driversz = drivers.withColumn('zone', find_zone_udf(col('latitude'), col('longitude'))).na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371b2d4-a57a-43cc-9cb0-e4a88be8bca8",
   "metadata": {},
   "source": [
    "In practice, we will save this dataframe in a .csv file in order to use it as a new starting point. Indeed, in PySpark, operations that do wide transformations (such as groupBy() and join(), that we will mainly use in this work) are really expensive to use on a dataframe on which some function that is not part of the pyspark.sql library has been applied (such as our udf find_zone). Note that there is no way to generate the column zone without using a custom function, or in other words, there is no way to generate this column using functions of pyspark.sql only. Since we will use the resulting dataframe a lot, this will save us precious time, even if the csv writing takes about 30min. For instance, the Question 1 takes 35 min to run if it must compute the zones and then groupby, whereas it takes 12s to groupby a brand new csv file with the additional column zone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24bcec0c-2aa2-43ea-8ef9-ff1ddb37f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#driversz.write.option(\"header\", True).csv('drivers_zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be454c49-9182-4a5c-9afa-62db9e93d036",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "driversZ =  spark.read.csv('drivers_zone.csv', header=True, inferSchema=True)\\\n",
    "                .select(['driver','timestamp','latitude','longitude','zone'])\\\n",
    "                .withColumn('zone', col('zone').cast(IntegerType())).na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6c31a5b-1548-4354-9e39-19ad3d3ea90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----------+----------+----+\n",
      "|  driver|          timestamp|  latitude| longitude|zone|\n",
      "+--------+-------------------+----------+----------+----+\n",
      "|c473205b|2017-08-31 14:24:25|-12.106778|-76.998078|  21|\n",
      "|a0f3b4e1|2017-08-31 14:24:26|-12.103913|-76.963727|  21|\n",
      "|1236f9fe|2017-08-31 14:24:26|-12.133777|-77.004266|  14|\n",
      "|ae4a06a2|2017-08-31 14:24:26|-12.085963|-76.987582|  21|\n",
      "|ab7a6c63|2017-08-31 14:24:26|-12.072973|-77.061448|  22|\n",
      "|5ee73484|2017-08-31 14:24:26|-12.067694|-77.068442|  22|\n",
      "|4fff90cb|2017-08-31 14:24:26|-12.144308|-76.989234|  14|\n",
      "|e9f90dfb|2017-08-31 14:24:10|-12.063665|-76.963254|  21|\n",
      "|c1719f8d|2017-08-31 14:24:10|-12.070187|-76.994167|  21|\n",
      "|4c299505|2017-08-31 14:24:11| -11.96459|-77.015983|  25|\n",
      "|49f033bd|2017-08-31 14:24:11|  -12.0906|-77.069808|  22|\n",
      "|749df32a|2017-08-31 14:24:11|-12.109964| -76.97535|  21|\n",
      "|f8d5c453|2017-08-31 14:24:11|-12.080683|-77.036188|  22|\n",
      "|914f7296|2017-08-31 14:24:11|-12.218541|-76.928484|  15|\n",
      "|ee744228|2017-08-31 14:24:30|-12.120202|-77.035704|  22|\n",
      "|353c20b9|2017-08-31 14:24:31|-12.119177|-76.997462|  21|\n",
      "|483e84f9|2017-08-31 14:24:31|-12.109511|-76.995862|  21|\n",
      "|e26ff40f|2017-08-31 14:24:31|-12.130376|-77.017779|  13|\n",
      "|3697a724|2017-08-31 14:24:31|-12.097054|-77.032223|  22|\n",
      "|45a9882d|2017-08-31 14:24:31|-12.016618|-77.004344|  20|\n",
      "+--------+-------------------+----------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- driver: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- zone: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "driversZ.show()\n",
    "driversZ.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b5c8c-f0d8-4215-aa8e-e582c1fba4b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "[back to Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b91580-6a42-4c91-9272-6e29f2687698",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ca126e-adc4-4902-b710-c58506e26b86",
   "metadata": {},
   "source": [
    "The goal of this question is to know what are the ten zones that are visited by the most drivers. The strategy is simply to exhibit the different pairs (driver, zone) that are in the dataframe and afterwards to group them by zone while counting how many unique drivers have been in each zone. It's a good idea to .cache() the result, since we will use it again later on in Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6089638b-d648-45d8-be1e-94be2490afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driversByZone = driversZ.dropDuplicates(['driver','zone']).groupBy('zone').count().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4b0dd1c-f097-419f-89f3-0958b815acbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:================================================>      (175 + 9) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|zone|count|\n",
      "+----+-----+\n",
      "|  22|10823|\n",
      "|  21| 8039|\n",
      "|  14| 4150|\n",
      "|  25| 3638|\n",
      "|  13| 3178|\n",
      "|  20| 1591|\n",
      "|  23| 1160|\n",
      "|  24| 1064|\n",
      "|  26|  747|\n",
      "|  15|  512|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 20.8 ms, sys: 4.49 ms, total: 25.3 ms\n",
      "Wall time: 11.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "driversByZone.orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f5e29-8f36-45a2-8591-7282802175d9",
   "metadata": {},
   "source": [
    "[back to Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fbdb2c-f798-44c3-8e70-8f4804865c1a",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "In this question, we want to adress some other features of the dataset. \n",
    "\n",
    "Firstly, we would like to know what are the average speed, the average travel time and the average travel distance for the drivers in a given zone. For this purpose, we will need to compute some differences between data of distinct rows. In PySpark, there is no notion of index for a row. However, a pyspark.sql function such as lead (with offset $= j$) allows us to access the would-be $(n+j)^{\\textrm{th}}$ row when computing the would-be $n^{\\textrm{th}}$ row, accounting that the dataset is ordered and partitioned. All in all, this can be done by defining a Window and by specifying how to partition and to order it. In this way, Spark can have some notion of index and position.\n",
    "\n",
    "Secondly, what about the exchanges between the zones ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05bbcf-794d-452a-94e8-74aae6df01d8",
   "metadata": {},
   "source": [
    "### 2.1 Dynamics\n",
    "\n",
    "Let us define the functions that we will need : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fea186c9-a31a-4015-87dd-b55597422cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance function \n",
    "def get_distance(lat1,long1,lat2,long2):\n",
    "    return geopy.distance.geodesic((lat1,long1),(lat2,long2)).m\n",
    "\n",
    "#Conversion to udf\n",
    "get_distance_udf = udf(get_distance, FloatType())\n",
    "\n",
    "\n",
    "#Time function \n",
    "def get_time(t1,t2):\n",
    "    if (t1 == None) or (t2 == None) :\n",
    "        return None\n",
    "    else:\n",
    "        return (t2-t1).seconds\n",
    "\n",
    "#Conversion to udf\n",
    "get_time_udf = udf(get_time, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be0f0dd-f979-4a36-b709-a8852a014def",
   "metadata": {},
   "source": [
    "In order the compute the \"instantaneous\" speeds, we have to compute the distance and the time gap between two successive GPS datapoints of the same driver. Hence, the natural way to specify the Window in which the lead function will run is the following : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5684385-88e1-4256-bf76-99a5ee04ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spec of the Window, in order to run the lead function\n",
    "windowSpec = Window.partitionBy('driver').orderBy('timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a87bf3-b520-489c-b867-976d0907d13f",
   "metadata": {},
   "source": [
    "In this way, the function lead will see each driver subset as a finite area beyond which it cannot pass. In particular, it will return a null value for the last row of a driver subset, where the ordering by timestamp ensures that the last row correponds to the latest GPS recording of the driver. It is again a good idea to get rid of the null values that can occur in several ways : either it's the last row of a driver hence there is no next quantites, or the speed is a null value due to a vanishing $\\Delta t$. The latter comes from some bugs of the GPS recordings (as we will explain just below) : indeed, it is unphysical to have 2 datapoints that share the same timestamp but that are 100m apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6e6678a-b56e-4ff0-a1a7-4d09a584ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics = driversZ.withColumn('next_lat', lead('latitude', offset=1).over(windowSpec)) \\\n",
    "    .withColumn('next_lng', lead('longitude', offset=1).over(windowSpec)) \\\n",
    "    .withColumn('next_time', lead('timestamp', offset=1).over(windowSpec)) \\\n",
    "    .withColumn('deltaT', get_time_udf('timestamp', 'next_time') ) \\\n",
    "    .withColumn('deltaX', get_distance_udf('latitude', 'longitude','next_lat','next_lng') ) \\\n",
    "    .withColumn('speed', col('deltaX')/col('deltaT')).na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5632794-7247-4cba-9891-6cbe71f6a88a",
   "metadata": {},
   "source": [
    "By inspecting the results, one realizes that some GPS recordings are outliers. Indeed, an .orderBy('speed') in decreasing order tells us that some instantaneous speeds are around 1500 m/s, and a non-negligible number of other speeds are impossible to reach by car. This feature is not a computation mistake nor a code mistake, it really belongs to the dataset : these are just bugs of the GPS. In order to obtain some average speeds which are realistic, we put a cut-off and we will throw away all the datapoints that have a speed bigger than 200 km/h. Working in m/s units, this upper bound is chosen to be 56 m/s. \n",
    "\n",
    "Also, we will also choose a cut-off for the $\\Delta t$. Sometimes there is a huge time gap ($\\sim$ hours) between two consecutive timestamps for the same driver. We assume this is not due to his travel and that the speed computed during this big time gap is not a speed of travel : the driver simply stopped the recording and restarted it somewhere else, hours later. Hence, one should get rid of this step. We choose 20min (=1200s) as an upper bound for the difference between two consecutive timestamps, in order to be sure that they are kinematically meaningful.\n",
    "\n",
    "Note that we chose to apply this filtering in this dynamical section only, the other sections are processed with the whole dataset (except that we dropped out the datapoints that are not in the zones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "154b72f0-4913-43e0-9a87-485ed52d4682",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamicsFinal = dynamics.filter((dynamics['speed'] <56.0) & (dynamics['deltaT'] < 1200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12c3d0cb-76bd-48c6-bd39-27a1632e2dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----------+----------+----+----------+----------+-------------------+------+----------+--------------------+\n",
      "|  driver|          timestamp|  latitude| longitude|zone|  next_lat|  next_lng|          next_time|deltaT|    deltaX|               speed|\n",
      "+--------+-------------------+----------+----------+----+----------+----------+-------------------+------+----------+--------------------+\n",
      "|001b6172|2017-08-31 17:32:06|-12.004618| -77.06468|  25|-12.004617|-77.064681|2017-08-31 17:32:33|    27|0.15523106| 0.00574929846657647|\n",
      "|001b6172|2017-08-31 17:32:33|-12.004617|-77.064681|  25|-12.004626|-77.064683|2017-08-31 17:34:19|   106| 1.0191461|0.009614585705523222|\n",
      "|001b6172|2017-08-31 17:34:19|-12.004626|-77.064683|  25|-12.004626|-77.064683|2017-08-31 17:34:25|     6|       0.0|                 0.0|\n",
      "|001b6172|2017-08-31 17:34:25|-12.004626|-77.064683|  25| -12.00542|-77.068807|2017-08-31 17:35:50|    85| 457.61472|   5.383702536190257|\n",
      "|001b6172|2017-08-31 17:35:50| -12.00542|-77.068807|  25|-12.005483|-77.069245|2017-08-31 17:36:02|    12|  48.20485|   4.017070770263672|\n",
      "|001b6172|2017-08-31 17:36:02|-12.005483|-77.069245|  25|-12.005496|-77.069302|2017-08-31 17:36:08|     6|  6.371734|  1.0619556903839111|\n",
      "|001b6172|2017-08-31 17:36:08|-12.005496|-77.069302|  25| -12.00562|-77.069628|2017-08-31 17:36:20|    12| 38.059425|  3.1716187795003257|\n",
      "|001b6172|2017-08-31 17:36:20| -12.00562|-77.069628|  25|-12.006175|-77.069554|2017-08-31 17:36:32|    12| 61.922016|   5.160168011983235|\n",
      "|001b6172|2017-08-31 17:36:32|-12.006175|-77.069554|  25|-12.006576|-77.069476|2017-08-31 17:36:38|     6| 45.165497|  7.5275828043619795|\n",
      "|001b6172|2017-08-31 17:36:38|-12.006576|-77.069476|  25|-12.008546|-77.069153|2017-08-31 17:36:56|    18| 220.74648|  12.263693067762587|\n",
      "|001b6172|2017-08-31 17:36:56|-12.008546|-77.069153|  25|-12.009867|-77.068935|2017-08-31 17:37:08|    12| 148.04793|  12.337327321370443|\n",
      "|001b6172|2017-08-31 17:37:08|-12.009867|-77.068935|  25|-12.010338|-77.069187|2017-08-31 17:37:18|    10| 58.888237|   5.888823699951172|\n",
      "|001b6172|2017-08-31 17:37:18|-12.010338|-77.069187|  25|-12.009556|-77.070849|2017-08-31 17:37:42|    24| 200.60068|     8.3583615620931|\n",
      "|001b6172|2017-08-31 17:37:42|-12.009556|-77.070849|  25|-12.007902|-77.071388|2017-08-31 17:38:06|    24| 192.15382|   8.006409327189127|\n",
      "|001b6172|2017-08-31 17:38:06|-12.007902|-77.071388|  25| -12.00828|-77.071834|2017-08-31 17:38:24|    18|  64.08959|   3.560532887776693|\n",
      "|001b6172|2017-08-31 17:38:24| -12.00828|-77.071834|  25| -12.00726|-77.072739|2017-08-31 17:39:12|    48| 149.81522|   3.121150334676107|\n",
      "|001b6172|2017-08-31 17:39:12| -12.00726|-77.072739|  25| -12.00726|-77.072739|2017-08-31 17:39:24|    12|       0.0|                 0.0|\n",
      "|001b6172|2017-08-31 17:39:24| -12.00726|-77.072739|  25|-12.007261|-77.072739|2017-08-31 17:39:36|    12|0.11062235|0.009218528866767883|\n",
      "|001b6172|2017-08-31 17:39:36|-12.007261|-77.072739|  25|-12.006872|-77.072967|2017-08-31 17:40:06|    30| 49.681465|   1.656048838297526|\n",
      "|001b6172|2017-08-31 17:40:06|-12.006872|-77.072967|  25|-12.006414|-77.073051|2017-08-31 17:40:19|    13|  51.48421|   3.960323920616737|\n",
      "+--------+-------------------+----------+----------+----+----------+----------+-------------------+------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 9.43 ms, sys: 6.63 ms, total: 16.1 ms\n",
      "Wall time: 17.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dynamicsFinal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fbf238-8bd0-49ff-93dc-2eea0ad1f7fa",
   "metadata": {},
   "source": [
    "The first things that we want to compute are the *average travel time* and *average travel distance* for each zone. For each driver, one should sum all the $\\Delta t$ belonging to the same zones. Same for the $\\Delta x$. Using a groupBy(['driver','zone']), we will exhibits all the unique pairs (driver,zone) and we will associate the total time and distance to them with the agg function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b694327-f3e9-4205-af3c-0ec65c3362b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dzoneDistTime = dynamicsFinal.groupBy(['driver','zone']).agg( sum('deltaX').alias('dtot') , sum('deltaT').alias('ttot')).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef160d5-76af-4b55-be4b-3c2775a49fc0",
   "metadata": {},
   "source": [
    "Now we simply have to groupBy the zones while summing all the distances and times, then to divide these by the number of drivers in each zone that we computed in Question 1. For this purpose, we join the column 'count' of Question 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf93ab9e-5c08-4898-a4f9-59fa0cae4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "averageQuantities = dzoneDistTime.groupBy('zone').agg(sum('ttot').alias(\"total_time\"), sum('dtot').alias('total_dist')) \\\n",
    "    .join(driversByZone, 'zone') \\\n",
    "    .withColumn('avg_time/driver (min)', col('total_time')/(60*col('count'))) \\\n",
    "    .withColumn('avg_dist/driver (km)', col('total_dist')/(1000*col('count')))\n",
    "\n",
    "averageQuantities = averageQuantities.select(['zone','avg_time/driver (min)','avg_dist/driver (km)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de92f6dc-c3ae-41d4-955b-9245b2fa7e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:==================================================>   (188 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+--------------------+\n",
      "|zone|avg_time/driver (min)|avg_dist/driver (km)|\n",
      "+----+---------------------+--------------------+\n",
      "|  22|    75.92330530660014|   14.95825744395522|\n",
      "|  21|   52.470497988970436|  11.178773470081994|\n",
      "|  14|   29.755004016064255|    6.89719232403659|\n",
      "|  27|   29.624458874458874|   6.292309179868985|\n",
      "|  11|   29.066666666666666|   8.745267377084742|\n",
      "|  16|   28.298816029143897|   6.543657695698078|\n",
      "|  25|   26.751736301997436|   6.696107607515276|\n",
      "|  26|    26.23119143239625|   5.479586937049127|\n",
      "|   8|              26.0625|   8.501636286875232|\n",
      "|  28|    23.62638888888889|   5.159186138508842|\n",
      "|  19|   22.100751366120218|   4.951896950200414|\n",
      "|  38|    20.24848484848485|  3.4727615119801327|\n",
      "|  23|    19.50846264367816|   4.777424736315237|\n",
      "|  20|   19.442698512465956|  4.1675496402618215|\n",
      "|  13|   19.344435703796936|   4.616003906233819|\n",
      "|  24|   18.449733709273183|   2.922989659491128|\n",
      "|  18|     18.0785641025641|   3.958002401680419|\n",
      "|  37|   17.429931972789117|  4.8839770184007225|\n",
      "|  15|   13.444889322916667|  3.6580007328134525|\n",
      "|  30|    12.61111111111111|   3.596926790828506|\n",
      "|  40|   12.326470588235294|   4.128118237722008|\n",
      "|  31|    12.17542735042735|  1.4988695824008722|\n",
      "|  17|    11.75906432748538|   2.083281552488605|\n",
      "|  29|   11.335576923076923|  2.8555806200687703|\n",
      "|  49|    8.684722222222222|  2.6786293389927596|\n",
      "|  12|   6.7994047619047615|   2.118051070667803|\n",
      "|  35|                6.595|  1.4005737886466085|\n",
      "|   7|    5.833333333333333| 0.09301459789276123|\n",
      "|   2|    4.294444444444444|   3.904337541739146|\n",
      "+----+---------------------+--------------------+\n",
      "\n",
      "CPU times: user 288 ms, sys: 117 ms, total: 405 ms\n",
      "Wall time: 7min 42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "averageQuantities.orderBy('avg_time/driver (min)', ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec6cb2d-ddff-42ef-8839-aa5efa64c2fb",
   "metadata": {},
   "source": [
    "Afterwards, one can compute the *average speed by zone*. One must be careful : the timestamps are not equally distributed, so that the average speed of a given driver in a given zone is not equal to the average of the \"instantaneous\" speeds that we computed. Instead, it is obviously given by the total travel distance divided by the total travel time of a driver in the given zone. We already computed it and cached it in the dzoneDistTime dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8cd8bf4-e74e-40ef-bb4b-d414f1fbb392",
   "metadata": {},
   "outputs": [],
   "source": [
    "averageSpeeds = dzoneDistTime.withColumn('dzoneSpeed', col('dtot')*3.6/col('ttot')) \\\n",
    "            .groupBy('zone').agg( mean('dzoneSpeed').alias('avg_speed (km/h)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c684b37c-3cd2-48f3-8daa-bae613f86829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:============================================>         (165 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|zone|  avg_speed (km/h)|\n",
      "+----+------------------+\n",
      "|   2| 70.19798352784623|\n",
      "|  11| 59.53952607743003|\n",
      "|   8|36.896887496332376|\n",
      "|  30|26.484407724141303|\n",
      "|  12| 25.27155985417999|\n",
      "|  15| 25.17200485499606|\n",
      "|  27|24.636710630752123|\n",
      "|  29| 22.82129088318582|\n",
      "|  20| 22.10889838694594|\n",
      "|  16| 20.99577766424281|\n",
      "|  40|20.345577486434095|\n",
      "|  28|20.187964746304303|\n",
      "|  37| 19.74989393166856|\n",
      "|  17|18.008349792442996|\n",
      "|  25|17.698093068482624|\n",
      "|  49| 17.51972791634245|\n",
      "|  19|17.134066348741296|\n",
      "|  13|16.805070382999883|\n",
      "|  18|16.777034642167383|\n",
      "|  24|16.686885571654198|\n",
      "|  14|15.711401011118559|\n",
      "|  26| 15.65981682608462|\n",
      "|  23|15.566055160976825|\n",
      "|  21|14.067287275992708|\n",
      "|  38|13.140226659040211|\n",
      "|  22|12.429955215420618|\n",
      "|  35|11.379321170849513|\n",
      "|  31| 11.10770715159875|\n",
      "|   7| 0.956721578325544|\n",
      "+----+------------------+\n",
      "\n",
      "CPU times: user 32.3 ms, sys: 8.08 ms, total: 40.4 ms\n",
      "Wall time: 1.54 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "averageSpeeds.orderBy('avg_speed (km/h)', ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e360e4-047a-41c0-9e03-3895b9d104ec",
   "metadata": {},
   "source": [
    "[back to Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2195c15-7093-4a43-b37d-ff7ab17a3102",
   "metadata": {},
   "source": [
    "### 2.2 Exchanges\n",
    "\n",
    "Here we would like to have a better understanding of how the zones are connected. For instance, let us take the $i^{\\textrm{th}}$ zone. We could ask ourselves : in average, how many different zones are visited by the drivers which pass through zone $i$ ? The second question that we want to adress is closely related but take the back and forth between zones into account. Now the question would be : in average, how many zone exchanges are made by the drivers which pass through zone $i$ ? \n",
    "\n",
    "Let us start with the amount of *visited zones* related to a given zone : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f1de90b-1e6e-48dd-bafe-23d6053184dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairsDriverZone = driversZ.dropDuplicates(['driver','zone']).select(['driver', 'zone'])\n",
    "HowManyZonesByDriver = pairsDriverZone.cache().groupBy('driver').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ff9b42a-fece-4a48-bbc4-8908f5b4c35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|zone|     NumberOfZones|\n",
      "+----+------------------+\n",
      "|  12|              6.25|\n",
      "|   8| 6.166666666666667|\n",
      "|  35|               5.3|\n",
      "|  49|              5.25|\n",
      "|  11|               5.0|\n",
      "|  40| 4.705882352941177|\n",
      "|  30| 4.333333333333333|\n",
      "|   2| 4.333333333333333|\n",
      "|  29| 4.288461538461538|\n",
      "|  16| 4.251366120218579|\n",
      "|  15|        4.08203125|\n",
      "|  27| 3.987012987012987|\n",
      "|  28| 3.966666666666667|\n",
      "|  37|3.9591836734693877|\n",
      "|  17|3.9298245614035086|\n",
      "|  24|3.9097744360902253|\n",
      "|  19|3.7950819672131146|\n",
      "|  13|3.7174323473882946|\n",
      "|  23| 3.603448275862069|\n",
      "|  20|3.5392834695160276|\n",
      "|  14| 3.399277108433735|\n",
      "|  25|3.2825728422210005|\n",
      "|  18| 3.236923076923077|\n",
      "|  31| 3.217948717948718|\n",
      "|  21| 3.055852717999751|\n",
      "|  26|2.9036144578313254|\n",
      "|  22|2.8590039730204193|\n",
      "|  38|2.3636363636363638|\n",
      "|   7|               2.0|\n",
      "+----+------------------+\n",
      "\n",
      "CPU times: user 35 ms, sys: 5.23 ms, total: 40.2 ms\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pairsDriverZone.join(HowManyZonesByDriver, 'driver') \\\n",
    "    .groupBy('zone').agg( mean('count').alias('NumberOfZones')).orderBy('NumberOfZones', ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1147a291-2916-45b9-8418-34658de20623",
   "metadata": {},
   "source": [
    "Now, the *exchanges* between a given zone and the other zones. Here, we have to keep track of people coming back in the same zones. We will use the lead function as for the speeds, in order to spot the rows in which there is a change of zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e26811cc-57cd-4d57-99b0-3d0e73f0a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driversZd = driversZ.select(['driver','timestamp','zone']).withColumn('next_zone', lead('zone', offset=1).over(windowSpec)).na.drop() \\\n",
    "        .withColumn('diffzone', (col('next_zone') - col('zone'))/(col('next_zone') - col('zone')))\n",
    "\n",
    "HowManyZonesExchByDriver = driversZd.groupBy('driver').agg(sum('diffzone').alias('SumDiffZone')) \\\n",
    "        .withColumn('SumDiffZone+1', col('SumDiffZone')+1)  #to count the first zone in the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68750a04-4bf4-49e2-a96d-47cce27a3e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:=========================>                                (4 + 5) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|zone| NumberOfZonesExch|\n",
      "+----+------------------+\n",
      "|  12|10.074074074074074|\n",
      "|   8|               9.5|\n",
      "|  11| 8.333333333333334|\n",
      "|  35| 7.444444444444445|\n",
      "|  30| 7.333333333333333|\n",
      "|  49|7.2727272727272725|\n",
      "|  15| 7.246913580246914|\n",
      "|  13| 7.241500962155228|\n",
      "|  16| 7.226993865030675|\n",
      "|  17|7.0754716981132075|\n",
      "|  24| 6.805443548387097|\n",
      "|  40| 6.733333333333333|\n",
      "|  14| 6.712426978226235|\n",
      "|  23|6.4586330935251794|\n",
      "|  29|            6.3125|\n",
      "|  27|6.2615384615384615|\n",
      "|  20| 6.237264480111654|\n",
      "|  28| 6.083333333333333|\n",
      "|  19| 5.873303167420814|\n",
      "|  37| 5.738095238095238|\n",
      "|  21| 5.724942975982826|\n",
      "|  25|  5.67685723020483|\n",
      "|  18| 5.473856209150327|\n",
      "|  22|  5.45071474290591|\n",
      "|  31| 5.166666666666667|\n",
      "|  26| 4.949832775919733|\n",
      "|   2| 4.333333333333333|\n",
      "|  38|             3.375|\n",
      "|   7|               2.0|\n",
      "+----+------------------+\n",
      "\n",
      "CPU times: user 16.6 ms, sys: 6.18 ms, total: 22.8 ms\n",
      "Wall time: 13.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pairsDriverZone.join(HowManyZonesExchByDriver, 'driver') \\\n",
    "    .groupBy('zone').agg(mean('SumDiffZone+1').alias('NumberOfZonesExch')).orderBy('NumberOfZonesExch', ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1333d29-66ef-4ba7-b110-23d948269f36",
   "metadata": {},
   "source": [
    "[back to Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1e03a-37e6-4b7a-a211-f13fdecf02a3",
   "metadata": {},
   "source": [
    "## Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2d74dfc-0fde-4630-ad8b-2848e5ee4868",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff7029-8fe7-4879-a4d2-e42a6e033b4a",
   "metadata": {},
   "source": [
    "[back to Index](#Index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
